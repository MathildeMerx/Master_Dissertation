\documentclass[11pt]{article}

\usepackage[a4paper,left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{helvet}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\setlength{\parindent}{4em}
\setlength{\parskip}{1em}

\urlstyle{same}
\renewcommand{\familydefault}{\sfdefault}


\begin{document}

\begin{onehalfspace}


\title{%
Images Analysis of Time Series\\
\large Dissertation Methodology}
\date{{\today}}
\author{MERX Mathilde}

\maketitle 

\tableofcontents


\pagebreak

\section{Literature review}

\subsection{Algorithmic Financial Trading with Deep Convolutional Neural Networks: Time Series to Image Conversion Approach, \textit{by Omer Berat Sezera, Ahmet Murat Ozbayoglua}, 2018}

LOOK AT THE SECTION "RELATED WORK" for my own introduction.

15 diﬀerent technical indicators each with diﬀerent parameter selections are utilized. Each indicator instance generates data for a 15 day period. As a result, 15x15 sized 2-D images are constructed. 

The applications of deep neural networks on ﬁnancial forecasting models have been very limited. CNN models were mostly used, whereas they're commonly used for computer vision.

Each image is labelled "Sell" (highest point overall the 15 data points), "Buy" (lowest point overall the 15 data points) or "Hold" elsewise. Since Hold is much more frequent, it made the algorithm less accurate when used on "Sell" or "Buy" data. 

\subsection{Deep Learning and Time Series-to-Image Encoding for Financial Forecasting, \textit{by Barra et al.}, 2020}

Classification-based models have better results in forecasting than regression-based models.

Prediction on the S\&P500 index only, with CNNs and GADF.Then, tested against the buy\&hold strategy. 

They create their images by aggregating the data in 4 different ways (1-hour, 4-hours, 8-hours and 20- hours intervals) and assembling the 4 images hence obtained: right/left top/bottom corner. Advise a small network, because else there may be overfitting (simplified version of the VGG-16 and no ResNet34 for instance). Conv layers activated by ReLU, softmax for the classification layer.

Label: 1 if the next day opening is higher than the current day closing, 0 otherwise.

IDEA: TAKE 4 DATA PER DAY (i.e. min, max, opening, closing), 16 DAYS IN A ROW, then assemble the 4 images hence obtained: right/left top/bottom corner.

They used 20 CNNs to predict by or sell, with different initialization methods and seeds. If too many networks disagree, they just hold. That is to avoid the initialization problems only one network would entail.

They outperformed B\&H, whatever the 7-year period (event 2009-2016, a very profitable period).

\subsection{Deep-Gap: A deep learning framework for forecasting crowdsourcing supply/demand gap based on imaging time series and residual learning, \textit{by Said and Erradi}, 2019}

They use GADF, GASF and RP in CNN to forecast the supply/demand gap in crowdsourcing. Only two layers, to avoid the overfitting a bigger network 
might entail. ReLU between layers as well.

\subsection{Duality between Time Series and Networks, \textit{Campanharo et al.}, 2011}

Explains how to go from time series to a network, with a transition matrix. The first example is the MTF. They use 20 quantiles for 320 time stamps, so maybe not a good idea for my case? Necessity to use stationary data.

\subsection{Encoding Temporal Markov Dynamics in Graph for Visualizing and Mining Time Series, \textit{Liu and Wang}, 2018}

MTF represents the temporality evolution of our time series. The main diagonal of the matrix shows the small changes, whereas the top right and bottom left corners show the massive changes. A MTF image helps understand the volatility of the stock.

\subsection{Extracting Texture Features for Time Series Classiﬁcation, \textit{Souza, Silva and Batista}, 2014}

They used a recurrence plot, using the definition with the Heaviside function.

IDEA FOR MY DISSERTATION: USE MORLET WAVELETS. show different RP for different kind of time series (random noise, random walk, and periodic functions for instance).

Using feature extraction to grasp the texture of the RP. For instance, some kind of wavelet transform (Gabor).

Eventually, they did classification with SVM.

\subsection{Encoding Time Series as Images for Visual Inspection and Classiﬁcation Using Tiled Convolutional Neural Network, \textit{by Want and Oates}, 2015}

Tiled CNN for small datasets.

GAF interesting because bijective. $G = \tilde{X}' \tilde{X} - \sqrt{I - \tilde{X^2}} '\sqrt{I - \tilde{X^2}}$, where $\tilde{X}$ is the row vector of the rescaled time series, and $I$ is the unit row vector.

I HAVE THE CODE FOR THE NN (ResNet)

MTF doesn't work as well as GAF, has a bigger error rate, probably because its inverse is not clear. 

\subsection{Imaging Time-Series to Improve Classiﬁcation and Imputation, \textit{by Wang and Oates}, 2015}

GAF is bijective, and  provides a way to preserve temporal dependency. They cumulated GASF (cosines of the sum), GADF (sinus of the difference) and MTF as the three channels of an image.

At the last layer of the Tiled CNN, they used a linear soft margin SVM. Images have no "edges" or "angles", that's why Tiled CNN are preferred to CNNs.

They use auto-encoders to extract features from time series.

\subsection{Forecasting with time series imaging, \textit{by Li, Kang and Li}, 2020}

RP, then the bag-of-features model calculates the distribution characteristics of feature points in the whole image and then generates a global histogram. 

It's feature extraction. CNN are compared to BoF. They seem to work better.

\subsection{SAX-VSM: Interpretable Time Series Classiﬁcation Using SAX and Vector Space Model, \textit{by Seninand alinchik}, 2013}

Nearest neighbor algorithm is simple, accurate and robust, depends on a very few parameters and requires no training. However, the 1NN technique has a number of signiﬁcant disadvantages: inability to offer any insight into the classiﬁcation results, needs for a signiﬁcantly large training set representing a within-class variance in order to achieve desired accuracy, computationally expensive. 

Bag of patterns model to characterize the TS.

\subsection{Time series classification through visual pattern recognition, \textit{by Jastrebska}, 2019}

Encoding the TS to a 2D array: $(z_1, dz_1), ..., (z_n, dz_n)$ where $dz_n = z_n - z_{n-1}$. Hence using the first derivative of the data.

Only plotting the TS is no good idea: too many pixels carry no information. So we want images where each pixel is fully loaded with info, but as light as possible for computational reason.

\subsection{How much information is contained in a recurrence plot?, \textit{by Thiel, Romano and Kurths}, 2004}

They use only black and white (no grey scale: Heaviside function), because of image size. To criticize! We want to use ReLU. Threshold $\epsilon$ for the noise. Their purpose is to reconstruct the TS from the RP. MY IDEA (see paper 1.7): The inverse of a RP can be rather well estimated, so it can explain why it works well in CNNs.

\subsection{Time Series Classiﬁcation Using Compression Distance of Recurrence Plots, \textit{by Silva, Souza and Batista}, 2013}

Distances are massively used to compare two time series: the Euclidian distance, and the DTW (dynamic time warping). We can make an analogy between RP and the frequency analysis of sound.

10\% of the biggest value can be a good threshold for the RP.

TODO: COMPARE MY FINAL ALGORITHM WITH 1NN - DTW distance METHOD (with the gain: $\frac{accuracy(method_{new})}{accuracy(method_{1NN})})$.

\subsection{Time Series Classification Using Multi-Channels
Deep Convolutional Neural Networks, \textit{by Zheng et al.}, 2014}

Compared to 1-NN with DTW, the traditional feature-based classification methods are usually more efficient but less effective.

\subsection{Tiled convolutional neural networks, \textit{by Le et al.}, 2010}

CNNs have tied weights, which makes them trainable with much fewer data than other network. However, because of the way weights are tied, CNNs can only capture translational invariances, and not rotations for instance. Hence, instead of hard coding the translational invariances, it is better to le the network learn the invariances on its own, with for instance a tile weight tying scheme.

A tiled CNN constrains all units that are $k$ steps away to each other to be tied. When $k=1$, we have a CNN, when $k=+\infty$, we have an united layer.

\pagebreak

\section{Introduction}

Prediction has interested mankind for millenniums: as early as 3,000BC, Indians were trying to develop meteorology \cite{meteo}. Indeed, knowing the future temperatures and precipitations were needed for the crops. As society became more complex and more specialized, new jobs have required new types of forecasting. For instance merchants need to predict their sales, electrical engineers need to predict the electricity demand, and so on. 

It can be noted that all these objects of prediction (temperature, sales, electricity demand) can be translated into numbers which depend on time. That is what time series are: data (figures) indexed on time. As we've just noticed, time series forecasting is an area of great interest, in many different fields. In our case, we'll focus on time series forecasting in a financial context, and more precisely \textbf{forecasting next day price of each S\&P500 company stocks}.

There are many different tools for time series forecasting: mathematical and statistical analysis, among which ARIMA, regressions, curve fitting, Bayesian analysis, etc \cite{campanharo}. Unfortunately, these methods have their limits. One of the biggest is that they often fail to predict market surges or falls. A possible explanation is that most econometric models are made under the assumptions data are stationary, and follow a Gaussian \cite{arima}. 

Another (expending) tool used for time series forecasting is Machine Learning. In this case, people mostly use Long-Short Term Memory layers, or Convolutional Neural Network \cite{conv_lstm}. In both cases (statistical analysis and Machine Learning), data used for forecasting are the raw data: the numbers. However, it is interesting to note that a time series can also be described in images. For instance, the plot (or graphic) of the time series is an image. In our case, we have decided to focus on the graphical dimension of time series.

We want to focus on this dimension because nowadays, highly efficient computer vision algorithms are made. For instance, on the MNIST dataset (a classification problem of hand-written digits), we can now achieve an accuracy of over 99.70\%. It means when shown a hand-written digit (between 0 and 9), over 99.70\% of the time the algorithm will make a correct inference! 

Since there are very powerful computer vision algorithms, we have decided to try and exploit their strengths. As stated earlier, there is a very famous way of translating a time series into an image: the plot. Unfortunately, even though this representation is very telling for us, humans, it has a great default: it carries scarcely any info compared with its size \cite{jastrebska}. For each column of pixels in the image, only one pixel is activated. For a 32$\times$32 pixel image, it means only a little bit more than 3\% of the pixels carry information!

Consequently, if we are to forecast time series by using an image representation, we will need and find another type of image. An idea is to highlight the time dependency of the data. For instance, if at time $t_1$ a stock price is $x_{t_1} = X$ and at time $t_2$ the same stock price is $x_{t_2} = X$ as well, then what is the probability of $x_{t_1 + 1} = x_{t_2 + 1}$? This is the general idea of the Markov Transition Fields \cite{wang}. Other imaging techniques will be discussed later on.

Using images of time series should improve the forecasting for two reasons: first, imaging time series is a way to transform them, and maybe reveal some features: for instance time dependencies. Second, image classification algorithms are among the best nowadays. Thus we want to try and adapt our problem so that we can use these algorithms.

Even though time series forecasting is considered a regression problem, we can translate it to a classification problem. For instance, for stock price forecasting, a reliable algorithm inferring whether the stock price will increase or decrease tomorrow can be all that is needed. For more precision, the problem could be translated to: tomorrow, will the stock price:
\begin{itemize}
    \item increase by more than 0.5\%?
    \item increase by 0.1\% to 0.5\%?
    \item increase by less than 0.1\% or decrease by less than 0.1\%?
    \item decrease by 0.1\% to 0.5\%?
    \item decrease by more than 0.5\%?
\end{itemize}

Obviously, the problem can be refined to as many categories as needed. Thus it is possible to use a classification computer vision algorithm to forecast a time series. 

In the next part, we will review the literature available on the matter "imaging time series to improve time series forecasting". Then, we will explain the methodology used to carry the research out. Afterwards, we will present our results, and discuss them. Eventually, we will conclude on this research paper.

\pagebreak

\section{Literature Review}

\label{sect_littrev}

In the remainder of section \ref{sect_littrev}, $x_t$ will be the value of our time series at time $t$, $M$ will be the matrix which will be translated to an image, and $M_{i,j}$ will be the value of the $i$-th row and $j$-th column element of matrix $M$. $M$ will have a size $(n \times n)$.

\subsection{Defining the values of our time series}

The first question we need to answer is: what are the values of the time series we will use? Will we use daily values, or weekly values? Will we use the raw stock price, or their first differentiate? 

Barra et al. \cite{barra}, who were focusing on improving time series imaging techniques, had a very interesting and novel idea. Instead of imaging just one time series (so using $x_t,...,x_{t+n-1}$), they suggested it could be interesting to define four different time series, then make one image for each of them, and eventually aggregating these four images in one image (each corner being the image of one time series). Their definition for these four time series was the following: their values were actually the stock prices of the \emph{same} company (so they only used the values of the time series $(x_t)$, they didn't introduce any new time series). But the difference resided in the time laps between each value of the time series. The first time series was the value of the company stock collected every hour ($x_t,...,x_{t+n-1}$), in the second one the values were collected every 4 hours ($x_t, x_{t + 4}, ...,x_{t+4 \times (n-1)}$), every 8 hours for the third image ($x_t, x_{t + 8}, ...,x_{t+8 \times (n-1)}$), and eventually every day for the fourth one ($x_t, x_{t + 24}, ...,x_{t+24 \times (n-1)}$). It is a way to highlight different periods and trends in the company stock price. 

Kwiatkowski et al. \cite{stationary} advise using a transformation of the time series, so as to make it stationary. The idea is to obtain a time series which joint probability distribution does not change when shifted in time. This will result in the mean and the variance (among other) not depending on time. The different transformations they suggest are the first or second differentiate (using $x_t - x_{t-1}$ or $x_t - x_{t-2}$), the logarithm ($\log(x_t)$), or a combination of both ($\log(x_t) - \log(x_{t-1})$). The latter is particularly useful when imaging time series: this way, we are not dealing with the stock price, but with (the logarithm of) its evolution percentage. With this method, companies which stock price are especially expensive won't be dealt with differently than companies which stock are cheap, whereas it would be the case with the first or second differentiate.

\subsection{Imaging time series techniques}

Now, we need to image our time series. What techniques can we use? Jastebska \cite{jastrebska} highlights the impossibility to simply plot the function $f(t) = x_t$ where $t$ is the time and $x_t$ the time series. Indeed, for each column of pixels only one would give information, whereas we want to \emph{maximize} the level of information per pixel. Hence, the idea is to operate some calculus revealing specific aspects of the time series, put the results in a matrix, and plot it as an image.

\subsubsection{Recurrence plot and wavelet transform}

Historically, a recurrence plot (RP) was an image described by the matrix: 

\begin{equation}
    M_{i,j} = \begin{cases}
        1 & \text{if } x_i = x_j \\
        0 & \text{else}
    \end{cases}
\end{equation}

As we can see, this was only coding a black and white image. Then, a more sophisticated variance emerged: $M_{i,j} = \Theta(\epsilon - |x_i - x_j|)$, where $\Theta(x) = 1$ if $x \geq 0$, else $\Theta(x) = 0$ ($\Theta$ is the Heaviside function), and $\epsilon$ is a constant to determine. This variance gave a grey scale image. 

The use of RP is quite old. In 2004, Thiel, Romano and Kurths \cite{thiel} were already suggesting using RP to represent time series. They were only using the old black and white version because it was giving lighter images than the grey scale version. They wanted to represent time series in images, and then being able to reconstruct the time series from the image. This idea of bijectivity between the time series and the image is fundamental for effective classification algorithm, as we will see in subsection \ref{gaf}. 

As time went on, and memory became less of a problem, researchers used more the Heaviside version of the RP: Senin and Malinchik in 2013; Silva, Souza and Batista in 2013 \cite{silva} and again (Souza, Silva and Batista) in 2014 \cite{souza}, Zheng et al in 2014 \cite{zheng}, Said and Erradi in 2019 \cite{senin}, and eventually Li, Kang and Li in 2020 \cite{li}. They were all using RP to image time series, for different reasons (usually classifying time series, but it could as well be feature extraction, or data reconstruction).

They all underline one great quality of the RP: it is a powerful feature highlighter. Silva, Souza and Batista \cite{silva} compare RP to frequency analysis when dealing with sound. However, when using RP for classification purposes, Zheng et al. \cite{zheng} explain it is more efficient, but less effective than Dynamic Time Warping (an algorithm used to compare two time series). So as to increase RP effectiveness in classification, Souza, Silva and Batista \cite{souza} used Gabor wavelets. This is a type of wavelet transform, which enables feature highlighting in an image. Using these wavelets answers to many researchers, who noticed RP weren't always highlighting time series patterns enough: Li, Kang and Li \cite{li}, or Senin and Malinchik \cite{senin}. 

Eventually, Silva, Souza and Batista \cite{silva} focus on the $\epsilon$ term of the RP defined with Heaviside function. They explain it should be used as a threshold so as to get rid of the noise: they suggest the definition $\epsilon = 0.1 \times \max\limits_t x_t$.

\subsubsection{Markov Transition Field}

In 2011, Campanharo et al \cite{campanharo} wanted to encode time series in networks. To that extend, they needed a transition matrix: the Markov Transition Field (MTF). It is defined as follows: let $n$ be the size of the MTF, and $p$ the number of samples from our time series we will be using ($x_t,...,x_{t+p-1}$). Then, we create the sequence $(q_i)$, where $q_0 = \min\limits_j x_i$, $q_n = \max\limits_j x_i$, and $q_i$ are the $n-1$ $n$-quantiles. For each sample $x_j / j \in [t;t+p-1]$  of our time series, we assign the value $X_j = \max\limits_{i \in [|0;n|]} \{ i / q_i \leq x_j\}$ (its quantile bin). Eventually, the coefficients of the MTF will be: $M_{i,j} = \sum\limits_{k=1}^{n} P(X_k = j | X_{k-1} = i)$. In words, this is the conditional probability that when a sample is in the bin $i$, the next sample will be in the bin $j$. Hence we directly have $\sum\limits_{i=0}^{n-1} M_{i,j} = 1$. 

The MTF was only supposed to be a transition matrix (its name is telling in that regard), but it has been used on its own. Indeed, in 2015, Wang and Oates published a series of articles (among which \cite{wang} and \cite{wang_encod}) using MTF for time series classification. They explain this encoding technique helps representing the temporal evolution of the time series. Around the main diagonal of the MTF, we can see the small changes of the time series, whereas in the top-right and bottom-left corner we can see the massive changes. Hence, MTF representation helps highlight the volatility of the stock. 

Said and Erradi \cite{said} then used this idea in 2019 to forecast a time series: the supply/demand gap in crowdsourcing. The use of MTF in their algorithm deep-gap gave them better results than with the LSTM architecture (RMSE of 11\% against 16\%), or even ARIMA (RMSE 11\% against 13\%).

However, Campahnaro et al. \cite{campanharo} advised using a lot of data samples compared to the matrix size (for instance $n = 20$ and $p = 320$), which isn't realistic: in our case, it would require over a yea of data to make a simple prediction! Hence, Wang and Oates \cite{wang_encod} suggested using MTF with \nameref{gaf}.

\subsubsection{Gramian Angular Field}
\label{gaf}

In 2015, Wang and Oates (\cite{wang_encod} and \cite{wang}) created a new imaging technique: the Gramian Angular Field (GAF). Here is its definition: first, we need to rescale our time series ($x_t, ..., x_{t+n-1}$) from -1 to 1. Let $m_+ = \max\limits_{i \in [t; t+n-1]} x_t$, and $m_- = \min\limits_{i \in [t; t+n-1]} x_t$. We create the sequence $X_i = \frac{2 \times x_i - m_+ - m_-}{m_+ - m_-}$, and $X_i \in [-1;1], \forall i$. Then, we create the sequence $\phi_i = \arccos(X_i)$. Since $X_i \in [-1;1]$ this is possible, and $\phi_i \in [0;\pi], \forall i \in [t;t+n-1]$. Eventually, for $M$ the matrix of the GAF, $M_{i,j} = \cos(\phi_i + \phi_j)$.

This representation has many advantages. First, it is bijective: the main diagonal is enough to find back the sequence $(X_i)$, and having only two values of the stock between $t$ and $t+n-1$ is enough to have all the values $x_t, ..., x_{t+n-1}$. Wang and Oates stress the importance of having "bijective" imaging techniques: algorithms have much better results than with images with which the time series can't be found back. Second, the GAF encodes some features of the time series: the correlation (in the coefficients $M_{i,j / |i-j| = k}$ we have the relative correlation by superposition of direction with respect to time interval $k$).

On their own, training classification algorithms with GAF gave rather positive results, depending on the time series used. Trained on electrocardiogram data for instance, Wang and Oates \cite{wang_encod} obtained 11\% error rate, compared with 21\% for the MTF. However, what brought even better results, was using GAF and MTF together, as several color channels of the image. In that case, Wang and Oates \cite{wang} obtained a 9\% error rate, still on the electrocardiogram dataset.

This idea of different images superposed as channels of the image has been re-used several times: for instance, Said and Erradi tried in 2019 \cite{said} to superpose GAF and RP images.

\subsection{Classification techniques}

Barra et al. \cite{barra} wanted to forecast the S\&P500 index. Their purpose was to have an algorithm predicting whether the index would increase or decrease the next day. Since they didn't have a lot of data, they stated having one very deep network would be too prone to overfitting. Hence, they wanted having limited size models (for instance a simplified version of VGG-16, and not ResNet34). This architecture being chosen, they didn't want the results to depend on the initialization: they wanted to ensure a proper convergence of their algorithm. So as to address this problem, they decided training \emph{20} models, each of them having the same architecture but a different initialization. Then, if more than $n$\% of the models agree on "increase" or "decrease", it is the overall prediction. Else, the overall prediction is "unsure". $n$ is an hyperparameter, $50 \leq n \leq 70$. Berat Sezera and Ozbayoglua \cite{berat} tried using directly three classes ("increase", "decrease" and "stagnate"), but their algorithm tended to forecast stagnation too often (it had a recall of 55\% - see \ref{eval}). Barra et al. \cite{barra} had more promising results, which will be detailed in \ref{eval}.

Their simplified version of VGG-16 Barra et al. used is constituted by 5 CNN layers, and a fully connected one. Indeed, CNN are a type of network commonly used for image classification. In 2010, Le et al. \cite{le} suggested an improvement of the CNN: the tiled CNN. In a CNN, the image is abstracted to a feature map in which all the weights are tied together. The idea of Tiled CNN is to only tie weights which are $k$ steps from each other ($k$ being an hyper parameter). When $k = 1$, it becomes a regular CNN. 

Tiled CNN let more the model learn by itself: instead of hard-coding the translational invariances, the network will find them out on its own. Wang and Oates \cite{wang} used Tiled CNN to obtain more successful classification algorithm. However the code they suggest today is that of a simplified ResNet, with regular CNN layers. They only use a dozen of CNN layers, with the same intention as Barra et al.'s: avoiding overfitting. 

\subsection{Evaluating the model}
\label{eval}

In this article \cite{ouannes}, Ouannès explains the most basic evaluation for classification is the accuracy: $\frac{n_{well}}{n_{tot}}$, where $n_{well}$ is the number of prediction the algorithm got right, and $n_{tot}$ is the total number of prediction made by the algorithm. Unfortunately, this metric isn't always relevant: if one class is abundantly more represented than the other(s), the accuracy would be very high if the algorithm was to always predict this very class, which is not the outcome we want. Hence, we need new metrics: the precision $P$ and the recall $R$. For a binary classification (0 or 1), let:

\begin{itemize}
    \item $T_0$ be the number of 0 correctly predicted
    \item $T_1$ be the number of 1 correctly predicted
    \item $F_0$ be the number of 0 the algorithm mistakenly predicted as 1
    \item $F_0$ be the number of 1 the algorithm mistakenly predicted as 0
\end{itemize}

Then, $R = \frac{T_0}{T_0 + F_1}$: that is the percentage of 0 that are rightly predicted by the algorithm. $R = \frac{T_0}{T_0 + F_0}$: that is the percentage of times the algorithm is right when it predicts 0. This is a first way, numeric, to evaluate one algorithm.

Barra et al. \cite{barra} also suggested a more financial evaluation: comparing the predictions of their algorithm with the Buy\&Hold strategy. The idea is the following: many great investors (among which Warren Buffett) emphasize the best strategy when doing long term trading is to constitute a diversified portfolio, and then keep the values of this portfolio for as long as possible. For instance, despite the 2008 crisis and the coronavirus outbreak, the S\&P500 index has more than doubled between 2007 and today. Hence, for their testing data, Barra et al. compared these two strategies:

\begin{itemize}
    \item Buying a share of the S\&P500 at the beginning of the testing period, hold on to it, and see its value at the end of the period
    \item When the algorithm predicts "increase", buy a long action (buy a share of the index, then sell it before the market closes). When the algorithm predicts "decrease", buy a short action (sell a share of the index, then buy it before the market closes). If the algorithm predicts "unsure", do nothing. 
\end{itemize}

What they noticed is that their strategy (relying on their algorithm) was on average better than the Buy\&Hold strategy. Furthermore, when they were doing poorer than Buy\&Hold, it was only a little bit, whereas when they were doing better it was with a much bigger margin. 

\pagebreak

\section{Methodology}

\subsection{Data collection}

The purpose was to have a lot of data. Indeed, Machine Learning algorithms require massive amounts of data for the training, so I needed to constitute a large dataset. To that extent, the S\&P500 is very interesting: one year of stock price history represents around 125,000 samples!

The data used are the S\&P500 505 company stock prices, from January 1st 2015 until December 31st 2019. To be more precise, there are 500 companies, which issued 505 common stocks.

\subsubsection{Company names}

So as to obtain the name of the 505 component stocks from the S\&P500, one can use the list from Wikipedia \cite{500names}.

\pagebreak

\begin{thebibliography}{9}
    \bibitem{meteo}
    Wikipedia. Timeline of Meteorology. Retrieved on 02/08/2020 from \url{https://en.wikipedia.org/wiki/Timeline_of_meteorology}

    \bibitem{campanharo} 
    Campanharo et al., 2011, \textit{Duality between Time Series and Networks}

    \bibitem{arima}
    Petrica, Stancu, Tindeche. 2016. \textit{Limitation of ARIMA models in financial and monetary economics}

    \bibitem{conv_lstm}
    Wan et al. 2019 \textit{Multivariate Temporal Convolutional Network: A Deep Neural Networks Approach for Multivariate Time Series Forecasting}

    \bibitem{jastrebska}
    Agnieszka Jastebska. 2019. \textit{Time series classification through visual pattern recognition}

    \bibitem{wang}
    Wang et Oates. 2015. \textit{Imaging Time-Series to Improve Classification and Imputation}

    \bibitem{barra}
    Barra et al. 2020. \textit{Deep Learning and Time Series-to-Image Encoding for Financial Forecasting}

    \bibitem{stationary}
    Kwiatkowski et al. 1992. Testing the null hypothesis of stationarity against the alternative of a unit root: How sure are we that economic time series have a unit root? \textit{Journal of Econometrics}, 54(1-3), 159–178

    \bibitem{thiel}
    Thiel, Romano and Kurths. 2004. \textit{How much information is contained in a recurrence plot?}

    \bibitem{senin}
    Senin and Malinchik. 2013. \textit{SAX-VSM: Interpretable Time Series Classification Using SAX and VectorSpace Model}

    \bibitem{silva}
    Silva, Souza and Batista. 2013. \textit{ Time  Series  Classification  Using  Compression  Distance  of  RecurrencePlots}

    \bibitem{souza}
    Souza, Silva and Batista. 2014. \textit{Extracting Texture Features for Time Series Classification}

    \bibitem{zheng}
    Zheng et al. 2014. \textit{Time Series Classification Using Multi-Channels Deep Convolutional Neural Networks}

    \bibitem{li}
    Li, Kang and Li. 2020. \textit{Forecasting with time series imaging}

    \bibitem{said}
    Said and Erradi. 2019.\textit{Deep-Gap: A deep learning framework for forecasting crowdsourcing supply/demand  gap  based  on  imaging  time  series  and  residual  learning}

    \bibitem{wang_encod}
    Wang and Oates. 2015. \textit{Encoding Temporal Markov Dynamics in Graph for Visualizing and Mining Time Series}

    \bibitem{le}
    Le et al. 2010. \textit{Tiled convolutional neural networks}

    \bibitem{ouannes}
    Ouannès. Precision and Recall. Retrieved on 08/08/2020 from \url{https://pouannes.github.io/blog/precision-recall/}

    \bibitem{berat}
    Berat Sereza and Ozbayoglua. 2018. \textit{Algorithmic Financial Trading with Deep Convolutional Neural Networks:Time Series to Image Conversion Approach}

    \bibitem{500names}
    Wikipedia. List of S\&P500 companies. Retrieved on 09/08/2020 from \url{https://en.wikipedia.org/wiki/List_of_S%26P_500_companies}

    \bibitem{fastai_head}
    Fastai. Vision.learner. Retrieved on 02/08/2020 from \url{https://docs.fast.ai/vision.learner.html#create_head}

\end{thebibliography}

\end{onehalfspace}

\end{document}

%To count words: go to perl command line (windows key), 
%then "cd C:\Program Files\MiKTeX 2.9\scripts\texcount", 
%then "texcount.pl C:\Mathilde\Classe\Kings\Dissertation\Report\MMerx_Dissertation.tex"