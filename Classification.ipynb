{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import\" data-toc-modified-id=\"Import-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import</a></span><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Data-download\" data-toc-modified-id=\"Data-download-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data download</a></span></li></ul></li><li><span><a href=\"#Building-the-model\" data-toc-modified-id=\"Building-the-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Building the model</a></span></li><li><span><a href=\"#Preparing-the-data\" data-toc-modified-id=\"Preparing-the-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preparing the data</a></span></li><li><span><a href=\"#Training-the-models\" data-toc-modified-id=\"Training-the-models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training the models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-1:-Random-normal\" data-toc-modified-id=\"Model-1:-Random-normal-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Model 1: Random normal</a></span></li><li><span><a href=\"#Model-2:-Uniform\" data-toc-modified-id=\"Model-2:-Uniform-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Model 2: Uniform</a></span></li><li><span><a href=\"#Model-3:-He-normal\" data-toc-modified-id=\"Model-3:-He-normal-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Model 3: He normal</a></span></li><li><span><a href=\"#Model-4:-Orthogonal\" data-toc-modified-id=\"Model-4:-Orthogonal-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Model 4: Orthogonal</a></span></li><li><span><a href=\"#Model-5:-LeCun-Uniform\" data-toc-modified-id=\"Model-5:-LeCun-Uniform-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Model 5: LeCun Uniform</a></span></li><li><span><a href=\"#Model-6:-Orthogonal\" data-toc-modified-id=\"Model-6:-Orthogonal-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Model 6: Orthogonal</a></span></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-model-at-a-time\" data-toc-modified-id=\"One-model-at-a-time-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>One model at a time</a></span></li><li><span><a href=\"#All-the-models-together\" data-toc-modified-id=\"All-the-models-together-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>All the models together</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import convert_to_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras import Input\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(813306)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('objs_train_label.pkl', 'rb') as f:\n",
    "    images_2010_2, labels_2010_2 = pickle.load(f)\n",
    "\n",
    "images_2010_2 = np.array(images_2010_2, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(input_shape, n_feature_maps, nb_classes, initial):\n",
    "    print ('build conv_x')\n",
    "    x = keras.layers.Input(shape=(input_shape))\n",
    "    conv_x = keras.layers.BatchNormalization()(x)\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps, 8, 1, padding='same', kernel_initializer=initial)(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps, 5, 1, padding='same', kernel_initializer=initial)(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps, 3, 1, padding='same', kernel_initializer=initial)(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps, 1, 1,padding='same', kernel_initializer=initial)(x)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
    "    y = keras.layers.Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same', kernel_initializer=initial)(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same', kernel_initializer=initial)(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same', kernel_initializer=initial)(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same', kernel_initializer=initial)(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
    "    y = keras.layers.Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same', kernel_initializer=initial)(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same', kernel_initializer=initial)(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same', kernel_initializer=initial)(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same', kernel_initializer=initial)(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
    "    y = keras.layers.Activation('relu')(y)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling2D()(y)\n",
    "    out = keras.layers.Dense(nb_classes, activation='softmax')(full)\n",
    "    print ('        -- model was built.')\n",
    "    return x, out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(images_2010_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images_2010_2, labels_2010_2, test_size=0.3)\n",
    "nb_classes = 2\n",
    "nb_epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "Y_train = keras.utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = keras.utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "x_train_mean = x_train.mean()\n",
    "x_train_std = x_train.std()\n",
    "x_train = (x_train - x_train_mean)/(x_train_std)\n",
    "x_test = (x_test - x_train_mean)/(x_train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Random normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 , y1 = build_resnet(x_train.shape[1:], 64, nb_classes, 'random_normal')\n",
    "model1 = keras.models.Model(inputs=x1, outputs=y1)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                  patience=50, min_lr=0.0001) \n",
    "hist = model1.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs, verbose=1, validation_data=(x_test, Y_test), callbacks = [reduce_lr])\n",
    "log = pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('model1_label.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Uniform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x2 , y2 = build_resnet(x_train.shape[1:], 64, nb_classes, 'uniform')\n",
    "model2 = keras.models.Model(inputs=x2, outputs=y2)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                  patience=50, min_lr=0.0001) \n",
    "hist = model2.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs, verbose=1, validation_data=(x_test, Y_test), callbacks = [reduce_lr])\n",
    "log = pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('model2_label.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: He normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x3 , y3 = build_resnet(x_train.shape[1:], 64, nb_classes, 'he_normal')\n",
    "model3 = keras.models.Model(inputs=x3, outputs=y3)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                  patience=50, min_lr=0.0001) \n",
    "hist = model3.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs, verbose=1, validation_data=(x_test, Y_test), callbacks = [reduce_lr])\n",
    "log = pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('model3_label.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x4 , y4 = build_resnet(x_train.shape[1:], 64, nb_classes, 'orthogonal')\n",
    "model4 = keras.models.Model(inputs=x4, outputs=y4)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "model4.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                  patience=50, min_lr=0.0001) \n",
    "hist = model4.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs, verbose=1, validation_data=(x_test, Y_test), callbacks = [reduce_lr])\n",
    "log = pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.save('model4_label.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: LeCun Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x5 , y5 = build_resnet(x_train.shape[1:], 64, nb_classes, 'lecun_uniform')\n",
    "model5 = keras.models.Model(inputs=x5, outputs=y5)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "model5.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                  patience=50, min_lr=0.0001) \n",
    "hist = model5.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs, verbose=1, validation_data=(x_test, Y_test), callbacks = [reduce_lr])\n",
    "log = pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.save('model5_label.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('model1_label.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_model('model2_label.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = load_model('model3_label.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = load_model('model4_label.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = load_model('model5_label.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('objs_sandp2.pkl', 'rb') as f:\n",
    "    images_sp, values_sp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_sp = (images_sp - x_train_mean)/(x_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_model(testing_images, testing_values, x_mean, x_std, mod):\n",
    "    TP = 0.0\n",
    "    TN = 0.0\n",
    "    FP = 0.0\n",
    "    FN = 0.0\n",
    "    iterations = len(testing_images)\n",
    "#     investment = [1000.0 for i in range(iterations + 1)]\n",
    "    investment = 1000.0\n",
    "    nb_classes = 2\n",
    "    nb_epochs = 2\n",
    "    \n",
    "    for i in range(iterations):\n",
    "#         if i%1 == 0:\n",
    "        if i%10 == 0:\n",
    "#             print(str(i) + '-th iteration on ' + str(iterations) + '. Investment at ' + str(investment[i]) + '.')\n",
    "            print(str(i) + '-th iteration on ' + str(iterations) + '.\\n' + str(investment))\n",
    "        prediction = mod.predict(np.array([testing_images[i]]))[0]\n",
    "        predict = np.argmax(prediction)\n",
    "\n",
    "        if prediction[predict] > 0.6:\n",
    "#             investment[i+1] = investment[i] * (1 - prediction[predict]) +\\\n",
    "#                               investment[i] * prediction[predict] *\\\n",
    "#                               (testing_values[i][1]/testing_values[i][0] if (predict == 1) \n",
    "#                               else testing_values[i][0]/testing_values[i][1])\n",
    "            investment = investment * (1 - prediction[predict]) +\\\n",
    "                         investment * prediction[predict] *\\\n",
    "                         (testing_values[i] if (predict == 0) \n",
    "                         else 1/testing_values[i])\n",
    "    \n",
    "        if predict == 1:\n",
    "            if testing_values[i] >= 1: \n",
    "                TP += 1\n",
    "            else: \n",
    "                FP += 1\n",
    "        else:\n",
    "            if testing_values[i] <= 1: \n",
    "                TN += 1\n",
    "            else: \n",
    "                FN += 1\n",
    "            \n",
    "        with open('testing3/objs_' + str(i + 240) + '.pkl', 'rb') as f:\n",
    "            images, labels = pickle.load(f)\n",
    "        \n",
    "        images = np.array(images, dtype=np.uint8)\n",
    "            \n",
    "        x_training, x_testing, y_training, y_testing = train_test_split(images, labels, test_size=0.3)\n",
    "        batch_size = int(max(min(x_training.shape[0]/10, 16), 1))\n",
    "\n",
    "        Y_training = keras.utils.to_categorical(y_training, nb_classes)\n",
    "        Y_testing = keras.utils.to_categorical(y_testing, nb_classes)\n",
    "\n",
    "        x_training = (x_training - x_mean)/(x_std)\n",
    "        x_testing = (x_testing - x_mean)/(x_std)\n",
    "        \n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                                                      patience=50, min_lr=0.0001) \n",
    "        \n",
    "        mod.fit(x_training, Y_training, batch_size=batch_size, epochs=nb_epochs, verbose=0, validation_data=(x_testing, Y_testing), callbacks = [reduce_lr])\n",
    "    return [TP, TN, FP, FN, investment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_model(images_sp, values_sp, x_train_mean, x_train_std, resnet)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
