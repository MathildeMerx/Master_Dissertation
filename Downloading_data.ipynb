{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import\" data-toc-modified-id=\"Import-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import</a></span></li><li><span><a href=\"#Companies\" data-toc-modified-id=\"Companies-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Companies</a></span></li><li><span><a href=\"#Scraping-the-desired-stock-prices\" data-toc-modified-id=\"Scraping-the-desired-stock-prices-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Scraping the desired stock prices</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dowloading the stock prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll write a script to download the S&P500 stock prices, from January 2015 until December 2019 (so 5 full years). I've been greatly helped by this $\\href{https://github.com/CNuge/kaggle-code/blob/master/stock_data/getSandP.py}{script}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import some functions and libraries. \n",
    "1. ``Beautifulsoup`` is to scrape the names of the 500 companies from the S&P500\n",
    "2. ``datetime`` will fix the start and end date of the data we'll download.\n",
    "3. ``futures`` will enable us to download several data in parallel, hence accelerating the process\n",
    "4. ``web`` will be a webscraping tool: it can extract data, and then save them in a dataframe. To use it, I first need to download the library ``pandas_datareader``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Here, I had to install some libraries which weren' available on my computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from concurrent import futures\n",
    "import pandas_datareader.data as web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll search for the S&P500 companies name in the Wikipedia page. I've noticed the names have the class \"external text\", hence I'm looking for these in the Wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "results = soup.find_all(class_=\"external text\")\n",
    "results[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When printing the results of my query, I see company code names can be 1 to 5 characters long. One out of two results don't give the name of the company, but are a link to a report. Hence, I'm interested in the 1 to 5 characters before \"< / a >\".  Below is a function taking the html we can see above in argument, and returning the company code name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_companies(list_scraping):\n",
    "    res = []\n",
    "    for x in list_scraping:\n",
    "        name = str(x)[-9:-4]                       #The company code name can be up to 4 letters\n",
    "        if name != 'ports':                         #We don't want the \"report\" lines\n",
    "            if '>' in name:                        #in case the company code name is less than 4 letters\n",
    "                res.append(name.split('>')[1])\n",
    "            else:\n",
    "                res.append(name)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_step = list_companies(results)\n",
    "first_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the last elements of our list are not what we want. The company code names are capital letters, so we only keep that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_companies(list_scraping):\n",
    "    res = list_scraping.copy()\n",
    "    booleen = res[-1].isupper()\n",
    "    while not(booleen):\n",
    "        res = res[:-1]\n",
    "        booleen = res[-1].isupper()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_P500 = remove_non_companies(first_step)\n",
    "S_P500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(S_P500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The S&P500 comprises 505 common stocks issued by 500 large-cap companies, so we've got just that. Now, we've got to scrape the prices of the stocks of those companies from 2015 until 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the desired stock prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the time gap for which we want to obtain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_time = datetime(2010, 1, 1)\n",
    "train_end_time = datetime(2014, 12, 31)\n",
    "\n",
    "test_start_time = datetime(2015, 1, 1)\n",
    "test_end_time = datetime(2019, 12, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_codes = [] #That's the list of the companies we'll have failed to obtain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stock_train(company_code): #We don't use a for loop because we'll want to parallelize the scraping\n",
    "    #We may not have scraped the right company code name, so we try in case there may be errors\n",
    "    try:\n",
    "        stock_df = web.DataReader(company_code,'yahoo', train_start_time, train_end_time)\n",
    "        stock_df.to_csv('training/' + company_code + '.csv')\n",
    "    except:\n",
    "        wrong_codes.append(company_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stock_test(company_code): #We don't use a for loop because we'll want to parallelize the scraping\n",
    "    #We may not have scraped the right company code name, so we try in case there may be errors\n",
    "    try:\n",
    "        stock_df = web.DataReader(company_code,'yahoo', test_start_time, test_end_time)\n",
    "        stock_df.to_csv('testing/' + company_code + '.csv')\n",
    "    except:\n",
    "        wrong_codes.append(company_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we parallelize the data scraping: it allows us to be much faster to obtain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = len(S_P500)\n",
    "\n",
    "with futures.ThreadPoolExecutor(workers) as executor:\n",
    "    res = executor.map(download_stock_train, S_P500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = len(S_P500)\n",
    "\n",
    "with futures.ThreadPoolExecutor(workers) as executor:\n",
    "    res = executor.map(download_stock_test, S_P500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "505 * 2 - len(wrong_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good news! We've managed to download the data from a total of 737 companies, which should be enough to train our model."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
